{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78b5839c",
   "metadata": {},
   "source": [
    "### **Library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5732729a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/somnio.kimm/.pyenv/versions/3.10.13/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Library\n",
    "# Time\n",
    "from timeit import default_timer as timer\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# File\n",
    "import warnings\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "import random\n",
    "import chardet\n",
    "\n",
    "# Numerical & Data Handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import math\n",
    "from typing import List, Callable, Union, Dict, Any, Tuple\n",
    "import itertools\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, TensorDataset\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "%matplotlib inline\n",
    "from sklearn.tree import plot_tree\n",
    "from scipy.optimize import curve_fit\n",
    "from mpl_toolkits.axes_grid1 import host_subplot\n",
    "import mpl_toolkits.axisartist as AA\n",
    "from pandas.plotting import lag_plot\n",
    "\n",
    "# Classic ML Libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Neural Network Libraries\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "from torch.nn import functional as F\n",
    "import tensorflow as tf\n",
    "from transformers import pipeline\n",
    "\n",
    "# NLP\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from transformers import AutoTokenizer, AutoModel, BertTokenizer, BertForSequenceClassification\n",
    "from datasets import Dataset\n",
    "\n",
    "# Feature Engineering\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import (StandardScaler, RobustScaler, MinMaxScaler, MaxAbsScaler, Normalizer,\n",
    "\t\t\t\t\t\t\t\t   LabelEncoder, OneHotEncoder, OrdinalEncoder, LabelBinarizer)\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import *\n",
    "from torchmetrics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6c2a01",
   "metadata": {},
   "source": [
    "### **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c08f9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(filepath_or_buffer=\"data/imdb/imdb_50k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029e305e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "\t\"\"\"\n",
    "    Tokenize a given text string by:\n",
    "    - Lowercasing all characters\n",
    "    - Removing HTML tags\n",
    "    - Removing non-alphabetic characters except whitespace\n",
    "    - Splitting text into individual word tokens\n",
    "\n",
    "    Args:\n",
    "        text (str): Raw input text\n",
    "\n",
    "    Returns:\n",
    "        list of str: Tokenized list of cleaned words\n",
    "    \"\"\"\n",
    "\n",
    "\ttext = text.lower()\n",
    "\ttext = re.sub(pattern=r\"<.*?>\", repl=\"\", string=text)\n",
    "\ttext = re.sub(pattern=r\"[^a-z\\s]\", repl=\"\", string=text)\n",
    "\n",
    "\treturn text.split()\n",
    "\n",
    "# Tokenize text\n",
    "reviews_tokenized = df[\"review\"].apply(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951a5ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding\n",
    "df[\"label\"] = df[\"sentiment\"].map({\"positive\": 1, \"negative\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8365bb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word counter\n",
    "counter = Counter()\n",
    "for tokens in reviews_tokenized:\n",
    "    counter.update(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868f0bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocab dict\n",
    "min_freq = 2 # Keep tokens with frequency >= 2\n",
    "vocab = {'<pad>': 0, '<unk>': 1} # Start vocab w/ special tokens\n",
    "tokens_sorted = sorted(counter.items(), key=lambda x: x[1], reverse=True) # Sort tokens by frequency\n",
    "for word, freq in tokens_sorted:\n",
    "    if freq >= min_freq:\n",
    "        vocab[word] = len(vocab) # Add a token to vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3036f093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_and_pad(tokens, max_len=256):\n",
    "    \"\"\"\n",
    "    Convert a list of word tokens into a fixed-length list of integer indices\n",
    "    using a predefined vocabulary. Pads shorter sequences with <pad> tokens\n",
    "    and truncates longer ones.\n",
    "\n",
    "    Args:\n",
    "        tokens (list of str): List of tokenized words\n",
    "        max_len (int): Desired fixed length of output sequence (default is 256)\n",
    "\n",
    "    Returns:\n",
    "        list of int: Encoded and padded sequence of length `max_len`\n",
    "    \"\"\"\n",
    "\n",
    "    vec_encoded = [vocab.get(token, vocab['<unk>']) for token in tokens] # Convert each word in tokens to its integer id from the vocab dictionary, and if a token is not found in vocab, use the index of '<unk>'\n",
    "    vec_encoded = vec_encoded[:max_len]\n",
    "    vec_padding = [vocab['<pad>']] * (max_len - len(vec_encoded))\n",
    "    return vec_encoded + vec_padding\n",
    "\n",
    "# Encode and pad\n",
    "MAX_LEN = 256\n",
    "reviews_encoded = reviews_tokenized.apply(lambda x: encode_and_pad(x, MAX_LEN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da61d69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data split\n",
    "X = torch.tensor(reviews_encoded.tolist())\n",
    "y = torch.tensor(df['label'].values)\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e21a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor -> TensorDataset (Wrap input and label) -> DataLoader (Divide dataset into mini-batches)\n",
    "BATCH_SIZE = 64\n",
    "dl_train = DataLoader(TensorDataset(X_train, y_train), batch_size=BATCH_SIZE, shuffle=True)\n",
    "dl_val = DataLoader(TensorDataset(X_val, y_val), batch_size=BATCH_SIZE)\n",
    "dl_test = DataLoader(TensorDataset(X_test, y_test), batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ade245",
   "metadata": {},
   "source": [
    "### **Classic ML**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20096f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of words\n",
    "vectorizer = CountVectorizer(max_features=5000)\n",
    "X_train_val_vec = vectorizer.fit_transform(X_train_val)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# Logistic Regression\n",
    "model_logistic_regression = LogisticRegression(max_iter=1000)\n",
    "model_logistic_regression.fit(X_train_val_vec, y_train_val)\n",
    "y_pred = model_logistic_regression.predict(X_test_vec)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5324ddcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_val_vec = vectorizer.fit_transform(X_train_val)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# Logistic Regression\n",
    "model_logistic_regression = LogisticRegression(max_iter=1000)\n",
    "model_logistic_regression.fit(X_train_val_vec, y_train_val)\n",
    "y_pred = model_logistic_regression.predict(X_test_vec)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3892abc",
   "metadata": {},
   "source": [
    "### **NN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cb9661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device agnostic code\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1395834e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_model_v0(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=vocab['<pad>'])\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        _, (hidden, _) = self.lstm(embedded)\n",
    "        return self.fc(hidden[-1]).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1480d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiation\n",
    "model = LSTM_model_v0(len(vocab), embedding_dim=256, hidden_dim=128).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Early stopping setup\n",
    "patience = 3\n",
    "counter = 0\n",
    "best_acc_val = 0.0\n",
    "best_model_state = None\n",
    "\n",
    "EPOCHS = 20\n",
    "for epoch in range(EPOCHS):\n",
    "    # Training\n",
    "    model.train()\n",
    "    correct_train, total_train = 0, 0\n",
    "    for batch_X, batch_y in dl_train:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.float().to(device)\n",
    "        y_logits = model(batch_X)\n",
    "        loss_train = criterion(y_logits, batch_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "        y_pred_prob = torch.sigmoid(y_logits)\n",
    "        y_pred = torch.round(y_pred_prob)\n",
    "        correct_train += (y_pred == batch_y).sum().item()\n",
    "        total_train += batch_y.size(0)\n",
    "    acc_train = correct_train / total_train\n",
    "    print(f\"Epoch {epoch+1}, Train Accuracy: {acc_train:.4f}\")\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    correct_val, total_val = 0, 0\n",
    "    with torch.inference_mode():\n",
    "        for batch_X, batch_y in dl_val:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            y_logits = model(batch_X)\n",
    "            y_pred_prob = torch.sigmoid(y_logits)\n",
    "            y_pred = torch.round(y_pred_prob)\n",
    "            correct_val += (y_pred == batch_y).sum().item()\n",
    "            total_val += batch_y.size(0)\n",
    "    acc_val = correct_val / total_val\n",
    "    print(f\"Epoch {epoch+1}, Val Accuracy: {acc_val:.4f}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if acc_val > best_acc_val:\n",
    "        best_acc_val = acc_val\n",
    "        counter = 0\n",
    "        best_model_state = model.state_dict()  # Save best model\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2965465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.inference_mode():\n",
    "    for batch_X, batch_y in dl_test:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        y_logits = model(batch_X)\n",
    "        y_pred_prob = torch.sigmoid(y_logits)\n",
    "        y_pred = torch.round(y_pred_prob)\n",
    "        correct += (y_pred == batch_y).sum().item()\n",
    "        total += batch_y.size(0)\n",
    "print(f\"Test Accuracy: {correct / total:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
