{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All about Iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ðŸ”¹ Step 1: Importing Libraries**\n",
    "- **Data Handling:** `numpy`, `pandas`\n",
    "- **Data Visualization:** `matplotlib.pyplot`, `seaborn`, `plot_tree`, `Line2D`, `profile_report`\n",
    "- **Data Preprocessing:** `StandardScaler`, `RobustScaler`, `MinMaxScaler`, `MaxAbsScaler`, `Normalizer`\n",
    "- **Machine Learning Models:** `DecisionTreeClassifier`, `RandomForestClassifier`, `ExtraTreesClassifier`, `LogisticRegression`, `KNeighborsClassifier`, `SVC`, `OneVsRestClassifier`\n",
    "- **Dimensionality Reduction:** `PCA`, `LDA`, `t-SNE`, `KernelPCA`\n",
    "- **Feature Selection:** `SelectKBest`, `RFE`, `SequentialFeatureSelector`\n",
    "- **Hyperparameter Optimization:** `GridSearchCV`, `RandomizedSearchCV`, `Hyperopt`\n",
    "- **Miscellaneous:** `pairwise_distances`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Required Libraries\n",
    "\n",
    "# Numerical & Data Handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import math\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "%matplotlib inline\n",
    "from sklearn.tree import plot_tree\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "# Machine Learning Libraries\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Feature Engineering\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, MaxAbsScaler, Normalizer, LabelEncoder, OneHotEncoder\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, mutual_info_classif, RFE, SequentialFeatureSelector\n",
    "\n",
    "# Dimensionality Reduction\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Hyperparameter Tuning with Bayesian Optimization\n",
    "from hyperopt import hp, tpe, fmin, Trials\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import pairwise_distances, classification_report, accuracy_score\n",
    "import statsmodels.api as sm\n",
    "from skimpy import skim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ðŸ”¹ Step 2: Load and Prepare Data**\n",
    "- Load the **Iris dataset** from `sklearn.datasets`\n",
    "- Convert it into a **Pandas DataFrame** for easier processing\n",
    "- Store **feature names** and **target class names**\n",
    "- Convert target variable `y` to **NumPy array (`y_array`)** for compatibility\n",
    "- Combine **features (`X`) and target (`y`) into a single DataFrame (`df`)** for EDA\n",
    "\n",
    "</n>\n",
    "\n",
    "- 150 samples of iris flowers (50 for each class)\n",
    "- X: 4 features\n",
    "\t- sepal length (cm)\n",
    "    - sepal width (cm)\n",
    "\t- petal length (cm)\n",
    "\t- petal width (cm)\n",
    "- y: 3 classes\n",
    "\t- Setosa (0)\n",
    "\t- Versicolor (1)\n",
    "\t- Virginica (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "iris = load_iris() \n",
    "feature_names = iris.feature_names # Feature column names\n",
    "target_names = iris.target_names # Class labels ['setosa', 'versicolor', 'virginica']\n",
    "X = pd.DataFrame(data=iris.data, columns=feature_names) # Array -> DataFrame\n",
    "y = pd.DataFrame(data=iris.target, columns=['target']) # Array -> DataFrame\n",
    "y_array = y.copy().to_numpy().ravel() # DataFrame -> Array\n",
    "df = pd.concat([X, y], axis=1) # Full dataset for visualization\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Data split\n",
    "X_train_array = np.array(X_train)\n",
    "y_train_array = np.array(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ðŸ”¹ Step 3: Data Standardization**\n",
    "- Prevents large-valued features from **dominating** the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing the Data\n",
    "scaler = StandardScaler()\n",
    "# scaler = MinMaxScaler()\n",
    "# scaler = RobustScaler()\n",
    "# scaler = MaxAbsScaler()\n",
    "# scaler = Normalizer()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=feature_names) # Convert back to DataFrame for better readability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ðŸ”¹ Step 4.1: Data Summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Shape\n",
    "print(f\"Feature Matrix Shape (X): {X.shape}\")  # (150, 4)\n",
    "print(f\"Target Vector Shape (y): {y.shape}\")   # (150, 1)\n",
    "\n",
    "# Basic Statistics of Features\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Types\n",
    "df.info()\n",
    "\n",
    "# Handle Missing Values\n",
    "df.isnull().sum() # Number of missing values per column\n",
    "# df.fillna(0, inplace=True) # Fill NaN W/ 0\n",
    "# df.fillna(df.mean(), inplace=True)  # Fill w/ mean\n",
    "# df.interpolate(method = 'linear', inplace=True) # Fill w/ interpolation\n",
    "# df.dropna(inplace=True) # Remove rows w/ NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report\n",
    "skim(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curse of Dimensionality\n",
    "\n",
    "# Average pair-wise L2-distance\n",
    "def average_pairwise_distance(data):\n",
    "    \"\"\"\n",
    "    Computes the average pairwise L2-distance (Euclidean distance) for a given dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - data: A NumPy array or Pandas DataFrame containing numerical features.\n",
    "\n",
    "    Returns:\n",
    "    - The mean of all pairwise Euclidean distances between data points.\n",
    "    \"\"\"\n",
    "    distances = pairwise_distances(data) # Compute pairwise distances\n",
    "    upper_triangle_indices = np.triu_indices_from(distances, k=1) # Get upper triangle indices (excluding diagonal)\n",
    "    return distances[upper_triangle_indices].mean() # Compute average pairwise distance\n",
    "\n",
    "# Dimensions\n",
    "dimensions = []\n",
    "average_distances = []\n",
    "for i in range(1, X.shape[1] + 1): # Iterate over increasing dimensions\n",
    "    X_reduced = X.iloc[:, :i] # Use only the first i dimensions\n",
    "    avg_distance = average_pairwise_distance(X_reduced) # Compute average pairwise distance\n",
    "    dimensions.append(i) # Store the number of dimensions\n",
    "    average_distances.append(avg_distance) # Store the computed distance\n",
    "    print(f\"Dimensions: {i}, Average Pairwise Distance: {avg_distance:.4f}\")\n",
    "    \n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(dimensions, average_distances, marker='o', color='b')\n",
    "plt.title(\"Curse of Dimensionality: Increasing Average Pairwise Distance\")\n",
    "plt.xlabel(\"Number of Dimensions\")\n",
    "plt.ylabel(\"Average Pairwise Distance\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ðŸ”¹ Step 4.2: Visualizing Feature Distributions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms\n",
    "X.hist(figsize=(10, 6), bins=10, edgecolor='black')\n",
    "plt.suptitle(\"Feature Distribution - Iris Dataset\")\n",
    "plt.show()\n",
    "\n",
    "# Boxplot for Outlier Detection\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=X)\n",
    "plt.title(\"Boxplot of Iris Features\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.show()\n",
    "\n",
    "# Joint Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(x=df.columns[0], y=df.columns[1], data = df, hue = 'target')\n",
    "plt.show()\n",
    "\n",
    "# Pair Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.pairplot(df, hue='target')\n",
    "plt.show()\n",
    "\n",
    "# Count Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.countplot(x=df.columns[4], data=df)\n",
    "# df['target'].value_counts().plot(kind='bar')\n",
    "plt.show()\n",
    "\n",
    "# Heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(df.iloc[:, :4].corr(), annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation - Iris\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_DESC(x):\n",
    "\t\"\"\"\n",
    "    Computes and prints the boxplot statistics including:\n",
    "    - Maximum whisker value (upper bound)\n",
    "    - Q3 (75th percentile)\n",
    "    - Q2 (Median)\n",
    "    - Q1 (25th percentile)\n",
    "    - Minimum whisker value (lower bound)\n",
    "    - Number of outliers\n",
    "\n",
    "    Parameters:\n",
    "    - x: A numerical array or Pandas Series (single feature column)\n",
    "    \n",
    "    Prints:\n",
    "    - Boxplot statistics\n",
    "    - Number of outliers in the data\n",
    "    \"\"\"\n",
    "\tQ2 = np.median(x)\n",
    "\tQ1 = np.percentile(x, 25)\n",
    "\tQ3 = np.percentile(x, 75)\n",
    "\tIQR = Q3 - Q1\n",
    "\twhisker_upper = min(max(x), Q3 + 1.5 * IQR)\n",
    "\twhisker_lower = max(min(x), Q1 - 1.5 * IQR)\n",
    "\tX = np.array(['max', 'Q3', 'Q2', 'Q1', 'min'])\n",
    "\tY = np.array([whisker_upper, Q3, Q2, Q1, whisker_lower])\n",
    "\toutliers = x[(x < whisker_lower) | (x > whisker_upper)]\n",
    "\tprint(X,'\\n', Y, '\\n', \"Number of outliers:\", len(outliers))\n",
    "\n",
    "box_DESC(df.iloc[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ðŸ”¹ Step 5.1: Feature Selection - Filter Method**\n",
    "- Select features based on their **statistical relevance to the target variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chi square\n",
    "chi_square = SelectKBest(score_func=chi2, k=2) # Select top 2 features\n",
    "X_chi_square = chi_square.fit_transform(X, y_array.copy())\n",
    "features_chi_square = X.columns[chi_square.get_support()]\n",
    "print(\"Selected Features:\", features_chi_square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANOVA (F-test)\n",
    "f_test = SelectKBest(score_func=f_classif, k=2) # Select top 2 features\n",
    "X_selected = f_test.fit_transform(X, y_array.copy())\n",
    "features_f_test = X.columns[f_test.get_support()]\n",
    "print(\"Selected Features:\", features_f_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mutual information\n",
    "mi_scores = mutual_info_classif(X, y_array.copy())\n",
    "df_mi = [pd.DataFrame({'Feature': X.columns, 'MI Score': mi_scores})\n",
    "                      .sort_values(by='MI Score', ascending=False)]\n",
    "print(df_mi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ðŸ”¹ Step 5.2: Feature Selection - Wrapper Method**\n",
    "- Select features by **iteratively testing different subsets** and evaluating their impact on model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFS\n",
    "model_rf = RandomForestClassifier()\n",
    "model_sfs = SequentialFeatureSelector(model_rf, n_features_to_select=2, direction='forward')\n",
    "X_rfe = model_sfs.fit_transform(X, y_array.copy())\n",
    "features_sfs = X.columns[model_sfs.support_]\n",
    "print(\"Selected Features by SFS:\", features_sfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFE\n",
    "model_dt = DecisionTreeClassifier()\n",
    "model_rfe = RFE(model_dt, n_features_to_select=2, step=1, verbose=2)\n",
    "X_rfe = model_rfe.fit_transform(X, y_array.copy())\n",
    "features_rfe = X.columns[model_rfe.support_]\n",
    "print(\"Selected Features by RFE:\", features_rfe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ðŸ”¹ Step 5.3: Feature Selection - Embedded Method**\n",
    "- Select features **during the model training** process by incorporating feature selection into the learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra tree\n",
    "model_etc = ExtraTreesClassifier()\n",
    "model_etc.fit(X, y_array.copy())\n",
    "\n",
    "# Feature importance from tree-based model\n",
    "feature_importances = [pd.DataFrame({'Feature': X.columns, 'Importance': model_etc.feature_importances_})\n",
    "                                    .sort_values(by='Importance', ascending=False)]\n",
    "print(\"Feature Importance using ExtraTreesClassifier:\\n\", feature_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ðŸ”¹ Step 6.1: Feature Extraction - Linear**\n",
    "- **Method:** `PCA(n_components=2)`\n",
    "- Converts high-dimensional data into **lower-dimensional** space while preserving variance\n",
    "\n",
    "</n>\n",
    "\n",
    "- **Method:** `LDA(n_components=2)`\n",
    "- Finds the best axes to **separate different classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-initialise y\n",
    "y = pd.DataFrame(iris.target, columns=['target']) # Data reload to prevent the modification of y after fit_transform\n",
    "y_copy = y.copy().squeeze().to_numpy().astype(int) # DataFrame -> Array\n",
    "\n",
    "# PCA\n",
    "model_pca = PCA(n_components=2)\n",
    "X_train_pca = model_pca.fit_transform(X_train_scaled)\n",
    "df_pca = pd.DataFrame(X_train_pca, columns=['PC1', 'PC2'])\n",
    "df_pca['target'] = iris.target\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "colors = ['navy', 'turquoise', 'darkorange']\n",
    "for color, i, target_name in zip(colors, [0, 1, 2], target_names): # Plot PCA results\n",
    "    plt.scatter(df_pca.loc[df_pca['target'] == i, 'PC1'], df_pca.loc[df_pca['target'] == i, 'PC2'], color=color, lw=2, label=target_name, alpha=0.6) \n",
    "loadings = model_pca.components_.T * np.sqrt(model_pca.explained_variance_) # Extract feature contributions\n",
    "for i, (x, y) in enumerate(loadings): # Plot vectors (arrows) for original features \n",
    "    plt.arrow(0, 0, x, y, color='black', width=0.02, head_width=0.1) # Scaling arrows\n",
    "    plt.text(x * 1.2, y * 1.2, feature_names[i], color='black', fontsize=12)\n",
    "plt.title('PCA of IRIS Dataset with Feature Vectors')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend(loc=\"best\", shadow=False, scatterpoints=1)\n",
    "plt.axhline(0, color='gray', linestyle='--', linewidth=0.5) # Add horizontal line\n",
    "plt.axvline(0, color='gray', linestyle='--', linewidth=0.5) # Add vertical line\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-initialise y\n",
    "y = pd.DataFrame(iris.target, columns=['target']) # Data reload to prevent the modification of y after fit_transform\n",
    "y_copy = y.copy().squeeze().to_numpy().astype(int) # DataFrame -> Array\n",
    "\n",
    "# Kernel PCA\n",
    "model_kernel_pca = KernelPCA(n_components=2, kernel=\"rbf\", gamma=15)\n",
    "X_kernel_pca = model_kernel_pca.fit_transform(X_train_scaled)\n",
    "df_kernel_pca = pd.DataFrame(X_kernel_pca, columns=['PC1', 'PC2'])\n",
    "df_kernel_pca['target'] = y_copy \n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(x=df_kernel_pca[\"PC1\"], y=df_kernel_pca[\"PC2\"], hue=df_kernel_pca[\"target\"], palette=\"viridis\", alpha=0.8)\n",
    "plt.title('Kernel PCA w/ Feature Vectors - Iris')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.axhline(0, color='gray', linestyle='--', linewidth=0.5)  # Add horizontal line\n",
    "plt.axvline(0, color='gray', linestyle='--', linewidth=0.5)  # Add vertical line\n",
    "plt.grid()\n",
    "plt.legend(title=\"Classes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-initialise y\n",
    "y = pd.DataFrame(iris.target, columns=['target']) # Data reload to prevent the modification of y after fit_transform\n",
    "y_copy = y.copy().squeeze().to_numpy().astype(int) # DataFrame -> Array\n",
    "\n",
    "# LDA\n",
    "model_lda = LDA(n_components=2)\n",
    "X_lda = model_lda.fit_transform(X_train_scaled, y_copy)\n",
    "df_lda = pd.DataFrame(X_lda, columns=['LD1', 'LD2'])\n",
    "df_lda['target'] = y\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "colors = ['navy', 'turquoise', 'darkorange']\n",
    "for color, i, target_name in zip(colors, [0, 1, 2], target_names): # Plot LDA results \n",
    "    subset = df_lda[df_lda['target'] == i]\n",
    "    plt.scatter(subset['LD1'], subset['LD2'], color=color, lw=2, label=target_name, alpha=0.8)\n",
    "loadings = model_lda.scalings_[:, :2]\n",
    "loadings /= np.linalg.norm(loadings, axis=1, keepdims=True)  # Normalise for better visualisation\n",
    "for i, (x, y) in enumerate(zip(loadings[:, 0], loadings[:, 1])):  \n",
    "    plt.arrow(0, 0, x * 2, y * 2, color='black', width=0.02, head_width=0.1)\n",
    "    plt.text(x * 2.2, y * 2.2, feature_names[i], color='black', fontsize=12)\n",
    "plt.title('LDA of IRIS Dataset with Feature Vectors')\n",
    "plt.xlabel('Linear Discriminant 1')\n",
    "plt.ylabel('Linear Discriminant 2')\n",
    "plt.legend(loc=\"best\", shadow=False, scatterpoints=1)\n",
    "plt.axhline(0, color='gray', linestyle='--', linewidth=0.5)  \n",
    "plt.axvline(0, color='gray', linestyle='--', linewidth=0.5)  \n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ðŸ”¹ Step 6.2: Feature Extraction - Non-Linear**\n",
    "- **Method:** `TSNE(n_components=2)`\n",
    "- Preserves the **local structure** of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-initialise y\n",
    "y = pd.DataFrame(iris.target, columns=['target']) # Data reload to prevent the persistence of variable\n",
    "y_array = y.squeeze().to_numpy().astype(int) # Convert 2-D matrix to 1-D array\n",
    "\n",
    "# t-SNE\n",
    "model_t_sne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "X_t_sne = model_t_sne.fit_transform(X_train_scaled)\n",
    "df_t_sne = pd.DataFrame(X_t_sne, columns=['component 1', 'component 2'])\n",
    "df_t_sne['target'] = y  # Add target labels\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "colors = ['pink', 'purple', 'yellow']\n",
    "for i, target_name in enumerate(np.unique(y)):  \n",
    "    subset = df_t_sne[df_t_sne['target'] == i]\n",
    "    plt.scatter(subset['component 1'], subset['component 2'], color=colors[i], label=target_names[i], alpha=0.7)\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "plt.legend()\n",
    "plt.title('t-SNE Visualization of Iris Dataset')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ðŸ”¹ Step 7: Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, X, y, title):\n",
    "    \"\"\"\n",
    "    Plots the decision boundary for the model.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Trained model\n",
    "    - X: Pandas DataFrame of training/test features (only 2 features allowed)\n",
    "    - y: Pandas Series of target labels\n",
    "    - title: Title for the plot\n",
    "    \"\"\"\n",
    "    # Convert Pandas DataFrame to NumPy arrays\n",
    "    X_np = X.to_numpy()\n",
    "    y_np = y.to_numpy()\n",
    "\n",
    "    # Define grid boundaries\n",
    "    x_min, x_max = X_np[:, 0].min() - 0.5, X_np[:, 0].max() + 0.5\n",
    "    y_min, y_max = X_np[:, 1].min() - 0.5, X_np[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                         np.linspace(y_min, y_max, 200))\n",
    "\n",
    "    # Predict class labels for each point in the grid\n",
    "    Z = model.predict(pd.DataFrame(np.c_[xx.ravel(), yy.ravel()], columns=X.columns))\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # Plot decision boundary\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.Set1)\n",
    "    plt.scatter(X_np[:, 0], X_np[:, 1], c=y_np, edgecolors='k', cmap=plt.cm.Set1)\n",
    "    \n",
    "    # Label axes using Pandas column names\n",
    "    plt.xlabel(X.columns[0])\n",
    "    plt.ylabel(X.columns[1])\n",
    "    legend_labels = [Line2D([0], [0], marker='o', color='w', markerfacecolor=plt.cm.Set1(i/len(target_names)), markersize=8, label=target_names[i]) for i in range(len(target_names))]\n",
    "    plt.legend(handles=legend_labels, title=\"Classes\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "# Select 2 features\n",
    "X_train_2D = X_train.iloc[:, :2] \n",
    "X_test_2D = X_test.iloc[:, :2]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression on 2 features\n",
    "model_lr_2D = OneVsRestClassifier(LogisticRegression())\n",
    "model_lr_2D.fit(X_train_2D, y_train)\n",
    "\n",
    "# Evaluation\n",
    "accuracy = model_lr_2D.score(X_test_2D, y_test)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Plot\n",
    "plot_decision_boundary(model_lr_2D, X_train_2D, y_train, 'Logistic Regression Decision Boundary - Iris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Nearest Neighbour\n",
    "model_knn = KNeighborsClassifier(n_neighbors=5)\n",
    "model_knn.fit(X_train_2D, y_train.values.ravel())\n",
    "\n",
    "# Evaluation  \n",
    "accuracy = model_knn.score(X_test_2D, y_test)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "plot_decision_boundary(model_knn, X_train_2D, y_train, title=\"K-Nearest Neighbour Decision Boundary - Iris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "model_svm = SVC(kernel='rbf')\n",
    "model_svm.fit(X_train_2D, y_train.values.ravel())\n",
    "\n",
    "# Evaluation  \n",
    "accuracy = model_svm.score(X_test_2D, y_test)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "plot_decision_boundary(model_svm, X_train_2D, y_train, title=\"Support Vector Machine Decision Boundary - Iris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree w/ Pruning\n",
    "model_dt = DecisionTreeClassifier(criterion='gini', \n",
    "                                  random_state=42,\n",
    "                                  max_depth=4,         # Limits tree depth\n",
    "                                  min_samples_split=5, # Minimum samples to split a node\n",
    "                                  min_samples_leaf=2,  # Minimum samples in a leaf\n",
    "                                  max_leaf_nodes=10)   # Limits total leaf nodes\n",
    "model_dt.fit(X_train, y_train)\n",
    "\n",
    "# Feature importance\n",
    "feature_importance_df = [pd.DataFrame({'Feature': X_train.columns,\n",
    "                                      'Importance': model_dt.feature_importances_})\n",
    "                                      .sort_values(by='Importance', ascending=False)]\n",
    "print(feature_importance_df)\n",
    "\n",
    "# Evaluation  \n",
    "accuracy = model_dt.score(X_test, y_test)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(15, 8))\n",
    "class_names = [str(cls) for cls in model_dt.classes_]  # Convert class labels to strings\n",
    "plot_tree(model_dt, filled=True, feature_names=X_train.columns, class_names=class_names)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ðŸ”¹ Step 8: Hyperparameter Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest model\n",
    "model_rf= RandomForestClassifier()\n",
    "\n",
    "# Hyperparameter for grid search and random search\n",
    "hyperparams = {'n_estimators': [10, 50, 100],   # Number of trees\n",
    "\t\t\t   'max_depth': [None, 10, 20],     # Maximum depth of trees\n",
    "    \t\t   'min_samples_split': [2, 5, 10], # Minimum number of samples to split an internal node\n",
    "    \t\t   'min_samples_leaf': [1, 2, 4],   # Minimum number of samples to be in a leaf node\n",
    "    \t\t   'bootstrap': [True, False]}      # Bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search (Exhaustive search)\n",
    "grid_search = GridSearchCV(estimator=model_rf, \t\n",
    "                           param_grid=hyperparams, \t\n",
    "                           cv=5)\n",
    "grid_search.fit(X, y_array)\n",
    "\n",
    "# Best hyperparameter\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "print(\"Best cross-validation score:\", grid_search.best_score_)\n",
    "\n",
    "# Best model\n",
    "best_model = grid_search.best_estimator_\n",
    "best_model.fit(X, y_array)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random search (Random trials)\n",
    "random_search = RandomizedSearchCV(estimator=model_rf, \n",
    "                                   param_distributions=hyperparams,\n",
    "                                   n_iter=10, \t\n",
    "                                   cv=5, \n",
    "                                   random_state=42)\n",
    "random_search.fit(X, y_array)\n",
    "\n",
    "# Best hyperparameter\n",
    "best_params = random_search.best_params_\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "print(\"Best cross-validation score:\", random_search.best_score_)\n",
    "\n",
    "# Best model\n",
    "best_model = random_search.best_estimator_\n",
    "best_model.fit(X, y_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter search space for Bayesian optimisation\n",
    "space = {'max_depth': hp.choice('max_depth', np.arange(1, 20, dtype=int)),\n",
    "         'min_samples_split': hp.quniform('min_samples_split', 2, 20, 1), # Ensures integer values â‰¥ 2\n",
    "         'min_samples_leaf': hp.quniform('min_samples_leaf', 1, 10, 1), # Ensures integer values â‰¥ 1\n",
    "         'criterion': hp.choice('criterion', ['gini', 'entropy'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    \"\"\"\n",
    "    Objective function for hyperparameter optimization of a Decision Tree Classifier.\n",
    "\n",
    "    Parameters:\n",
    "    - params: A dictionary containing hyperparameter values for 'min_samples_split' and 'min_samples_leaf'.\n",
    "\n",
    "    Returns:\n",
    "    - Negative cross-validated accuracy (to convert maximization to minimization).\n",
    "    \"\"\"\n",
    "    params['min_samples_split'] = int(params['min_samples_split']) # Ensure integer type\n",
    "    params['min_samples_leaf'] = int(params['min_samples_leaf'])  \n",
    "    clf = DecisionTreeClassifier(**params)  # Create a decision tree w/ given parameters\n",
    "    accuracy = cross_val_score(clf, X, y_array, cv=5).mean()  # Compute cross-validated accuracy\n",
    "    return -accuracy  # Return negative accuracy (convert maximisation problem to minimisation problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian optimisation\n",
    "# 1. Select an initial random set of hyperparameters.\n",
    "# 2. Evaluate them using the objective function.\n",
    "# 3. Use past evaluations to build a probability model of which hyperparameters are likely to perform best.\n",
    "# 4. Prioritise testing the most promising hyperparameters in the next iteration.\n",
    "# 5. Repeat this for 50 evaluations (max_evals=50).\n",
    "# 6. Find the best hyperparameter combination.\n",
    "trials = Trials()\n",
    "best_params = fmin(fn=objective,  \n",
    "                   space=space,      \n",
    "                   algo=tpe.suggest,  \n",
    "                   max_evals=50,     \n",
    "                   trials=trials)\n",
    "\n",
    "# Convert to proper data type\n",
    "criterion_mapping = ['gini', 'entropy']  # Fixed incorrect mapping\n",
    "best_params['criterion'] = criterion_mapping[best_params['criterion']]  \n",
    "best_params['min_samples_split'] = int(best_params['min_samples_split'])\n",
    "best_params['min_samples_leaf'] = int(best_params['min_samples_leaf'])\n",
    "\n",
    "# Best hyperparameters\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "\n",
    "# Best model\n",
    "best_model = DecisionTreeClassifier(**best_params)\n",
    "best_model.fit(X, y_array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
